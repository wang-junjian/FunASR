import gradio as gr
from funasr import AutoModel
import numpy as np
import os
import librosa
import torch


# --- 0. è®¾å¤‡æ£€æµ‹é€»è¾‘ ---
def get_available_devices():
    devices = ["cpu"]
    if torch.cuda.is_available():
        devices.append("cuda")
    if torch.backends.mps.is_available():
        devices.append("mps")
    
    # é»˜è®¤é€‰æ‹©é¡ºåºï¼šcuda > mps > cpu
    default_device = "cpu"
    if "cuda" in devices:
        default_device = "cuda"
    elif "mps" in devices:
        default_device = "mps"
    
    return devices, default_device

AVAILABLE_DEVICES, DEFAULT_DEVICE = get_available_devices()

# --- 1. æ¨¡å‹åˆ—è¡¨é…ç½® ---
MODELS = {
    "Paraformer-zh (ä¸­æ–‡é•¿è¯­éŸ³)": "iic/speech_paraformer-large-vad-punc_asr_nat-zh-cn-16k-common-vocab8404-pytorch",
    "SenseVoiceSmall (å¤šè¯­è¨€/æƒ…æ„Ÿ/äº‹ä»¶)": "iic/SenseVoiceSmall",
    "Nano (ä¸­/è‹±/æ—¥)": "FunAudioLLM/Fun-ASR-Nano-2512",
    "MLT-Nano (å¤šè¯­è¨€)": "FunAudioLLM/Fun-ASR-MLT-Nano-2512"
}

loaded_models = {}

def resample_audio(y, sr, target_sr=16000):
    if y.dtype == np.int16:
        y = y.astype(np.float32) / 32768.0
    elif y.dtype != np.float32:
        y = y.astype(np.float32)
    
    if sr != target_sr:
        y = librosa.resample(y, orig_sr=sr, target_sr=target_sr)
    return y

# --- 2. å®æ—¶è¯†åˆ«æ ¸å¿ƒé€»è¾‘ ---
class StreamASRService:
    def __init__(self):
        self.device = DEFAULT_DEVICE # åˆå§‹ä½¿ç”¨é»˜è®¤è®¾å¤‡
        self.sample_rate = 16000
        self.chunk_size = [0, 10, 5] 
        self.chunk_stride = self.chunk_size[1] * 960 
        self.asr_model = None

    def load_model(self, device):
        """å½“ç”¨æˆ·åˆ‡æ¢è®¾å¤‡æ—¶é‡æ–°åŠ è½½æµå¼æ¨¡å‹"""
        if self.asr_model is None or self.device != device:
            print(f"æ­£åœ¨åŠ è½½/åˆ‡æ¢æµå¼æ¨¡å‹è‡³è®¾å¤‡: {device}...")
            self.device = device
            self.asr_model = AutoModel(
                model="paraformer-zh-streaming", 
                device=self.device, 
                chunk_size_ms=600, 
                disable_pbar=True
            )

    def process_stream(self, audio_data, state, device):
        if audio_data is None:
            return "", state

        # ç¡®ä¿æ¨¡å‹åœ¨å½“å‰é€‰å®šè®¾å¤‡ä¸Š
        self.load_model(device)

        sr, y = audio_data
        if sr != self.sample_rate:
            y = resample_audio(y, sr, self.sample_rate)
        
        if y.ndim > 1:
            y = y.mean(axis=1)
        y = y.astype(np.float32)

        if state is None:
            state = {
                "asr_cache": {},
                "full_text": "",
                "buffer": np.array([], dtype=np.float32)
            }

        state["buffer"] = np.concatenate([state["buffer"], y])

        while len(state["buffer"]) >= self.chunk_stride:
            chunk = state["buffer"][:self.chunk_stride]
            state["buffer"] = state["buffer"][self.chunk_stride:]

            res = self.asr_model.generate(
                input=chunk,
                cache=state["asr_cache"],
                is_final=False,
                chunk_size=self.chunk_size,
                encoder_chunk_look_back=4,
                decoder_chunk_look_back=1,
            )

            if res and res[0].get('text'):
                state["full_text"] += res[0]['text']

        return state["full_text"], state

stream_service = StreamASRService()

# --- 3. ç¦»çº¿è¯†åˆ«å‡½æ•° ---
def transcribe_offline(audio_path, model_name, hotwords_str, device, use_itn, use_punc, use_speaker):
    if audio_path is None: return "è¯·å…ˆä¸Šä¼ éŸ³é¢‘ã€‚"
    
    hotwords_list = [line.strip() for line in hotwords_str.split('\n') if line.strip()]
    model_dir = MODELS[model_name]
    
    # å¦‚æœè®¾å¤‡æ”¹å˜æˆ–æ¨¡å‹æœªåŠ è½½ï¼Œé‡æ–°åŠ è½½
    cache_key = f"{model_dir}_{device}_{use_itn}_{use_punc}_{use_speaker}"
    if cache_key not in loaded_models:
        print(f"æ­£åœ¨åŠ è½½ç¦»çº¿æ¨¡å‹ {model_name} è‡³ {device}...")
        model_params = {"model": model_dir, "device": device, "trust_remote_code": True, "disable_pbar": True}
        if "Paraformer" in model_name:
            model_params["vad_model"] = "iic/speech_fsmn_vad_zh-cn-16k-common-pytorch"
            if use_punc:
                model_params["punc_model"] = "iic/punc_ct-transformer_cn-en-common-vocab471067-large"
            if use_speaker:
                model_params["spk_model"] = "iic/speech_campplus_sv_zh-cn_16k-common"
        loaded_models[cache_key] = AutoModel(**model_params)

    model = loaded_models[cache_key]
    
    gen_kwargs = {"input": audio_path, "language": "auto", "use_itn": use_itn, "itn": use_itn, "hotwords": hotwords_list}
    res = model.generate(**gen_kwargs)
    
    if res:
        result = res[0]
        if use_speaker and "sentence_info" in result:
            formatted_text = ""
            last_spk = None
            for item in result["sentence_info"]:
                speaker_id = item.get("spk", "æœªçŸ¥")
                text = item['text']
                if speaker_id != last_spk:
                    if formatted_text:
                        formatted_text += "\n\n"
                    formatted_text += f"ğŸ˜€ã€{speaker_id}ã€‘{text}"
                    last_spk = speaker_id
                else:
                    formatted_text += text
            return formatted_text.strip()
        return result.get("text", "æ— è¯†åˆ«ç»“æœ")
    
    return "è¯†åˆ«å¤±è´¥"

# --- 4. æ„å»º Gradio ç•Œé¢ ---
custom_css = """
#text_output textarea, #stream_output textarea {
    height: 65vh !important;
}
footer { display: none !important; }
"""

with gr.Blocks(title="FunASR ç»¼åˆè¯­éŸ³è¯†åˆ«å·¥å…·", css=custom_css) as demo:
    gr.Markdown("# ğŸ™ï¸ FunASR å®æ—¶/ç¦»çº¿ è¯­éŸ³è¯†åˆ«")

    # å…¨å±€è®¾ç½®åŒºåŸŸ
    with gr.Accordion("âš™ï¸ å…¨å±€è®¾ç½®", open=False):
        with gr.Row():
            device_selector = gr.Dropdown(
                choices=AVAILABLE_DEVICES, 
                value=DEFAULT_DEVICE, 
                label="è®¡ç®—è®¾å¤‡ (å·²è‡ªåŠ¨è¯†åˆ«æœ€ä½³é€‰é¡¹)"
            )
        with gr.Row():
            use_itn = gr.Checkbox(label="å¼€å¯ ITN (æ•°å­—è½¬å†™)", value=True)
            use_punc = gr.Checkbox(label="å¼€å¯æ ‡ç‚¹é¢„æµ‹", value=True)
            use_speaker = gr.Checkbox(label="å¼€å¯è¯´è¯äººè¯†åˆ«", value=False)

    with gr.Tabs():
        with gr.TabItem("ç¦»çº¿è¯­éŸ³è¯†åˆ«"):
            with gr.Row():
                with gr.Column():
                    audio_input = gr.Audio(sources=["upload", "microphone"], type="filepath", label="ä¸Šä¼ éŸ³é¢‘")
                    model_selector = gr.Dropdown(choices=list(MODELS.keys()), value=list(MODELS.keys())[0], label="é€‰æ‹©æ¨¡å‹")
                    hotwords_input = gr.Textbox(label="çƒ­è¯ (æ¯è¡Œä¸€ä¸ª)", placeholder="é˜¿é‡Œå·´å·´\näººå·¥æ™ºèƒ½", lines=3)
                    submit_btn = gr.Button("å¼€å§‹è¯†åˆ«", variant="primary")
                with gr.Column():
                    text_output = gr.Textbox(label="è¯†åˆ«ç»“æœ", show_copy_button=True, elem_id="text_output")

        with gr.TabItem("å®æ—¶è¯­éŸ³è¯†åˆ«"):
            gr.Markdown("å®æ—¶æ¨¡å¼ä½¿ç”¨ `paraformer-zh-streaming`ã€‚")
            with gr.Row():
                with gr.Column():
                    # è®¾ç½® streaming=Trueï¼Œtime_limit å†³å®šå›è°ƒé¢‘ç‡ï¼ˆç§’ï¼‰
                    stream_input = gr.Audio(sources=["microphone"], streaming=True, label="éº¦å…‹é£è¾“å…¥")
                    clear_btn = gr.Button("æ¸…ç©ºè®°å½•")
                with gr.Column():
                    stream_output = gr.Textbox(label="å®æ—¶è¯†åˆ«å†…å®¹", show_copy_button=True, elem_id="stream_output")
            
            stream_state = gr.State()

    # äº‹ä»¶ç»‘å®š - ç¦»çº¿
    submit_btn.click(
        fn=transcribe_offline, 
        inputs=[audio_input, model_selector, hotwords_input, device_selector, use_itn, use_punc, use_speaker], 
        outputs=text_output
    )

    # äº‹ä»¶ç»‘å®š - å®æ—¶
    stream_input.stream(
        fn=stream_service.process_stream,
        inputs=[stream_input, stream_state, device_selector],
        outputs=[stream_output, stream_state],
        show_progress="hidden"
    )

    # æ¸…ç©ºæŒ‰é’®åŠŸèƒ½
    def reset_state():
        return "", None
    clear_btn.click(fn=reset_state, outputs=[stream_output, stream_state])

if __name__ == "__main__":
    demo.launch(
        server_name="0.0.0.0",
        server_port=7860,
        debug=True,
    )
